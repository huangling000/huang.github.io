项目背景，目的：利用贝叶斯分类器对文档进行分类。有多个文件夹，每个文件夹是一个类，文件夹内存放所有属于该类型的文档。可以随机每一个类中三分之二的文档作为训练集，用于训练分类模型，剩下三分之一作为测试集，用于测试分类模型的准确性。在mapreduce框架上实现代码输出训练模型。

1 贝叶斯分类器



2 集群搭建

安装三台虚拟机→配置jdk→设置主机ip让主机都在同一个网段下，互相可以ping通→重命名主机→建立主机名到ip映射→配置ssh免密登录→下载安装包配置文件

3 实现流程

mapreduce的处理过程：

inputformat模块做map前的预处理（inputsplit与RecordReader）将分割好的数据交给map，map任务完成后输出一系列kv，对map的中间结果进行shuffle（分区，排序，合并，归并）交给reduce处理（从无序的kv到有序的<k,v-list>），reduce处理完成后将结果交给outputformat模块进行处理，outputformat模块验证输出目录是否存在以及结果类型是否符合配置文件中的结果类型，都满足则输出reduce结果到分布式文件系统。

shuffle在map端与reduce端的过程：

map端：输入数据执行map任务→写入缓存→溢写→文件归并

reduce端：领取数据→归并数据



预处理阶段：hadoop主要是针对大文件进行并行处理，不擅长处理小文件。（小文件：文件大小是小于block size的75%，则定义为小文件。但小文件不仅是指文件比较小，如果Hadoop集群中的大量文件略大于block size，同样也会存在小文件问题。）在本次任务中，每个文件夹内的文档只有几kb，首先需要将这些文档合并成一个大文档，再上传到hdfs中进行处理。最后即每一个文档包含这个类中的所有文档的内容，每一行为合并前每个文档的内容，每个单词之间用制表符隔开。

整个项目一共有两个mapreduce任务组成。第一个任务是为了计算p（c），即一个文档为c类的概率。p(c)=属于c的文档个数/总文档个数。所以第一个项目只需要统计每个类的文档数目，p(c)也就算出来了。

因此第一个任务map阶段首先采用默认的输入格式TextInputFormat，将文件的每一行作为一次输入，输入类型key为LongWritable,为每一行的偏移量，value为IntWritable，为改行的内容，即预处理前的一个文档的内容。通过获取当前分片的文件名得到当前统计的文档所属的类名，每处理一行输出key类型为Text，value类型为IntWritable的键值对，作为map的输出。输出的key表示类名，value则是常数1，用于计数。在reduce阶段，输入格式与map输出保持一致，key为Text类型，value则为IntWritable类型的迭代器，用于获取IntWritable集合并遍历统计。最后将key的value累加，输出，则得到每个类所对应的文档数

第二个任务是为了计算p(d|c)，p(d|c)=p(t|d)p(t|d)......，t独立存在，但为了防止浮点溢出，同时保证不会出现因为一个单词从来没有出现而导致整个p(d|c)为0的情况，首先等式两边取对数，可以发现我们最终只想比较大小，那么只需要计算logp(t|c)相加的和。p(t1|c)=（T1+1）/（T1+1)+(T2+1)+.....=（T1+1）/（（T1+T2+.....）+B），B为c类别中T的种类。

因此我们需要计算每个种类中一个单词出现的个数。但我们知道mapreduce并没有实现一个键值对类型的可序列化的类，因此需要继承writable接口重新编写了一个类<classname,word>，可用于表示一个类中的一个单词。在map阶段，仍然使用默认的输入格式TextInputFormat，将文件的每一行作为一次输入，输入类型key为LongWritable,为每一行的偏移量，value为IntWritable，为该行的内容，即预处理前的一个文档的内容。

在map过程中将value值进行切割得到一个个单词，并通过得到当前分片的名字获取类名后，组成一个个<classname,word>形式的键值对。map的中间输出key类型则设置为<classname,word>，内容存放得到的键值对，value类型为IntWritable，为常数1，用于计数，统计原理与统计文档原理差不多。reduce输出key类型为<classname,word>，value类型为IntWritable。

<classname,word>的实现主要需要实现构造函数，set(),getclassname(),getword(),write(),readFields(),equals(),hashcode(),toString(),compareTo()等几个函数，实现如下：

```
public class ClassWord implements WritableComparable<ClassWord>{

	private Text classname;
	private Text word;

	public ClassWord(){
		set(new Text(),new Text());
	}

	public ClassWord(String classname,String word){
		set(new Text(classname),new Text(word));
	}

	public ClassWord(Text classname,Text word){
		set(classname,word);
	}

	public void set(Text classname,Text word){
		this.classname = classname;
		this.word = word;
	}

	public Text getClassname(){
		return classname;
	}

	public Text getWord(){
		return word;
	}

	@Override
	public void write(DataOutput out) throws IOException{
		classname.write(out);
		word.write(out);
	}

	@Override
	public void readFields(DataInput in) throws IOException{
		classname.readFields(in);
		word.readFields(in);
	}

	@Override
	public boolean equals(Object o){
		if(o instanceof ClassWord) {
			ClassWord tp = (ClassWord) o;
			return classname.equals(tp.getClassname()) && word.equals(tp.getWord());
		}
		return false;
	}

	@Override
	public int hashCode(){
		return classname.hashCode() + word.hashCode();
	}

	@Override
	public String toString(){
		return classname + "\t" + word;
	}

	@Override
	public int compareTo(ClassWord o){
		int cmp = classname.compareTo(o.getClassname());
		if(cmp != 0){
			return cmp;
		}
		return word.compareTo(o.getWord());
	}
}


```



inputforamt抽象类有两个抽象方法：getsplits（）与createRecordReader（）。通过inputsplit，mp可以做到校验作业输入的正确性，将输入文件切割成逻辑分片，一个inputsplit被分配给一个独立的maptask，并提供RecordReader实现，获取split中的kv对提供给mapper使用。

List<InputSplit> getSplits()：计算得到分给map任务的inputsplit。

RecordReader<K,V> createRecordReader()：创建RecordReader（抽象类，需要具体实现），RecordReader计算分片始末位置，打开输入流读取<k,v>对。

4 相关分布式系统spark，对比，区别，应用场合

5 大规模计算的发展史，从批处理到现在，什么在推动他们演变

create table user_info ( id int not null auto_increment, name varchar(64) not null default, gender tinvint not null default(0),--1代表男性，0代表女性-- age int not null default(0), telephone varchar not null default(''), register_mode varchar not null default(''), third_party_id varchar(64) not null default(''), primary key(id) );